{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"The second architecture is similar to CBOW, but instead of predicting the current word based on the\n",
    "context, it tries to maximize classification of a word based on another word in the same sentence.\n",
    "More precisely, we use each current word as an input to a log-linear classifier with continuous\n",
    "projection layer, and predict words within a certain range before and after the current word. We\n",
    "found that increasing the range improves quality of the resulting word vectors, but it also increases\n",
    "the computational complexity. Since the more distant words are usually less related to the current\n",
    "word than those close to it, we give less weight to the distant words by sampling less from those\n",
    "words in our training examples.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokens = []\n",
    "    wrd = ''\n",
    "    \n",
    "    for txt in string:\n",
    "        if txt.isalpha():\n",
    "            wrd = wrd + txt\n",
    "        if txt.isalpha() == False:\n",
    "            if len(wrd) != 0:\n",
    "                tokens.append(wrd)\n",
    "            wrd = ''\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tks, ws):\n",
    "    tx = []\n",
    "    ty = []\n",
    "    \n",
    "    for ind in range(len(tks) - 2*ws):\n",
    "        dummy = []\n",
    "        ty.append(tks[ind + ws])\n",
    "        for i in range(ws):\n",
    "            dummy.append(tks[ind + i])\n",
    "        for i in range(ws):\n",
    "            dummy.append(tks[ind + ws + i + 1])\n",
    "        tx.append(dummy)\n",
    "        \n",
    "    return tx, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(wrds):\n",
    "    \n",
    "    wrds_set = set(wrds)\n",
    "    wrds_dict = {}\n",
    "    num = 0\n",
    "    \n",
    "    for i in wrds_set:\n",
    "        wrds_dict[i] = num\n",
    "        num += 1\n",
    "        \n",
    "    return wrds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(data)\n",
    "encoded = encode(tokens)\n",
    "t_x, t_y = generate(tokens, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
