{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"The second architecture is similar to CBOW, but instead of predicting the current word based on the\n",
    "context, it tries to maximize classification of a word based on another word in the same sentence.\n",
    "More precisely, we use each current word as an input to a log-linear classifier with continuous\n",
    "projection layer, and predict words within a certain range before and after the current word. We\n",
    "found that increasing the range improves quality of the resulting word vectors, but it also increases\n",
    "the computational complexity. Since the more distant words are usually less related to the current\n",
    "word than those close to it, we give less weight to the distant words by sampling less from those\n",
    "words in our training examples.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokens = []\n",
    "    wrd = ''\n",
    "    \n",
    "    for txt in string:\n",
    "        if txt.isalpha():\n",
    "            wrd = wrd + txt\n",
    "        if txt.isalpha() == False:\n",
    "            if len(wrd) != 0:\n",
    "                tokens.append(wrd)\n",
    "            wrd = ''\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tks, ws):\n",
    "    tx = []\n",
    "    ty = []\n",
    "    \n",
    "    for ind in range(len(tks) - 2*ws):\n",
    "        dummy = []\n",
    "        ty.append(tks[ind + ws])\n",
    "        for i in range(ws):\n",
    "            dummy.append(tks[ind + i])\n",
    "        for i in range(ws):\n",
    "            dummy.append(tks[ind + ws + i + 1])\n",
    "        tx.append(dummy)\n",
    "        \n",
    "    return tx, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(wrds):\n",
    "    \n",
    "    wrds_set = set(wrds)\n",
    "    wrds_dict = {}\n",
    "    num = 0\n",
    "    \n",
    "    for i in wrds_set:\n",
    "        wrds_dict[i] = num\n",
    "        num += 1\n",
    "        \n",
    "    return wrds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(data)\n",
    "encoded = encode(tokens)\n",
    "t_x, t_y = generate(tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hoter(x, y, enc):\n",
    "    \n",
    "    len_onehot = len(encoded)\n",
    "    \n",
    "    enc_tx = []\n",
    "    for lis in x:\n",
    "        dummy = []\n",
    "        for wrd in lis:\n",
    "            pos = enc[wrd]\n",
    "            ar = np.zeros(len_onehot)\n",
    "            ar[pos] = 1\n",
    "            dummy.append(ar)\n",
    "        enc_tx.append(dummy)\n",
    "    \n",
    "    enc_ty = []\n",
    "    for wrd in y:\n",
    "        pos = enc[wrd]\n",
    "        ar = np.zeros(len_onehot)\n",
    "        ar[pos] = 1\n",
    "        dummy.append(ar)\n",
    "        enc_ty.append(dummy)\n",
    "        \n",
    "    return enc_tx, enc_ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_x, one_hot_y = one_hoter(t_x, t_y, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.random((79, 10))\n",
    "w2 = np.random.random((10, 79))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrd_emb(inp, w):\n",
    "    \n",
    "    op = np.dot(inp, w)\n",
    "    op = op.mean(axis = 0)\n",
    "    \n",
    "    return op\n",
    "\n",
    "\n",
    "def dense(inp, w):\n",
    "    \n",
    "    op = np.dot(inp, w)\n",
    "    \n",
    "    return op\n",
    "\n",
    "\n",
    "def softmax(inp):\n",
    "    \n",
    "    op = np.exp(inp) / (np.sum(np.exp(inp)) + 0.00001)\n",
    "    \n",
    "    return op\n",
    "\n",
    "def forward_pass(x, w1, w2):\n",
    "    \n",
    "    op1 = wrd_emb(x, w1)\n",
    "    op2 = dense(op1, w2)\n",
    "    op3 = softmax(op2)\n",
    "    \n",
    "    return op3\n",
    "\n",
    "def cross_entropy(orig, pred):\n",
    "    \n",
    "    loss = -1 * np.sum(orig * np.log(pred))\n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
